---
title: agent-eval-kit
description: TypeScript-native eval framework for AI agent workflows
template: splash
hero:
  tagline: Record-replay, deterministic graders, LLM-as-judge, CI integration. Ship confident AI agents with $0 pre-push evals.
  actions:
    - text: Get Started
      link: /agent-eval-kit/getting-started/quick-start/
      icon: right-arrow
    - text: View on GitHub
      link: https://github.com/FlanaganSe/agent-eval-kit
      variant: minimal
      icon: external
---

import { Card, CardGrid, LinkCard, Tabs, TabItem } from '@astrojs/starlight/components';

## Why agent-eval-kit?

<CardGrid stagger>
  <Card title="Record & Replay" icon="pencil">
    Record API responses once, replay them instantly. Run evals in milliseconds with zero API costs during development.
  </Card>
  <Card title="20 Built-in Graders" icon="approve-check">
    From exact-match to hallucination detection. Compose graders with boolean logic, weight scores, and set quality gates.
  </Card>
  <Card title="LLM-as-Judge" icon="star">
    When deterministic graders aren't enough, use LLM rubric scoring with built-in caching, bias mitigation, and cost tracking.
  </Card>
  <Card title="CI-Ready" icon="rocket">
    Quality gates with configurable thresholds. Fail your CI pipeline when eval scores regress below acceptable levels.
  </Card>
</CardGrid>

## Quick Example

<Tabs>
  <TabItem label="eval.config.ts">
    ```typescript
    import { defineConfig } from "agent-eval-kit";
    import { contains, latency } from "agent-eval-kit/graders";

    export default defineConfig({
      suites: [
        {
          name: "smoke",
          target: async (input) => {
            const response = await myAgent(input.prompt);
            return { text: response.text, latencyMs: response.latencyMs };
          },
          cases: "cases/smoke.jsonl",
          defaultGraders: [
            { grader: contains("expected output") },
            { grader: latency(5000) },
          ],
          gates: { passRate: 0.9 },
        },
      ],
    });
    ```
  </TabItem>
  <TabItem label="cases/smoke.jsonl">
    ```json
    {"id": "greeting", "input": {"prompt": "Say hello"}, "expected": "hello"}
    {"id": "capital", "input": {"prompt": "What is the capital of France?"}, "expected": "Paris"}
    {"id": "math", "input": {"prompt": "What is 2 + 2?"}, "expected": "4"}
    ```
  </TabItem>
</Tabs>

```bash
# Record fixtures (calls your agent once)
agent-eval-kit run --mode=live --record --suite=smoke

# Replay from fixtures (instant, $0)
agent-eval-kit run --mode=replay --suite=smoke

# Watch mode for development
agent-eval-kit run --watch --mode=replay --suite=smoke
```

## Next Steps

<CardGrid>
  <LinkCard title="Quick Start" description="Run your first eval in under 5 minutes" href="/agent-eval-kit/getting-started/quick-start/" />
  <LinkCard title="Record & Replay" description="Capture responses once, replay them for $0 evals" href="/agent-eval-kit/guides/record-replay/" />
  <LinkCard title="Graders" description="20 built-in graders from exact-match to LLM rubrics" href="/agent-eval-kit/guides/graders/" />
  <LinkCard title="CI Integration" description="Quality gates that fail your pipeline on regressions" href="/agent-eval-kit/guides/ci-integration/" />
</CardGrid>
