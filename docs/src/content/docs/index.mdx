---
title: agent-eval-kit
description: TypeScript-native eval framework for AI agent workflows
template: splash
hero:
  tagline: Record-replay, deterministic graders, LLM-as-judge, CI integration. Ship confident AI agents with $0 pre-push evals.
  actions:
    - text: Get Started
      link: /agent-eval-kit/getting-started/quick-start/
      icon: right-arrow
    - text: View on GitHub
      link: https://github.com/FlanaganSe/agent-eval-kit
      variant: minimal
      icon: external
---

import { Card, CardGrid } from '@astrojs/starlight/components';

## Why agent-eval-kit?

<CardGrid stagger>
  <Card title="Record & Replay" icon="pencil">
    Record API responses once, replay them instantly. Run evals in milliseconds with zero API costs during development.
  </Card>
  <Card title="20 Built-in Graders" icon="approve-check">
    From exact-match to hallucination detection. Compose graders with boolean logic, weight scores, and set quality gates.
  </Card>
  <Card title="LLM-as-Judge" icon="star">
    When deterministic graders aren't enough, use LLM rubric scoring with built-in caching, bias mitigation, and cost tracking.
  </Card>
  <Card title="CI-Ready" icon="rocket">
    Quality gates with configurable thresholds. Fail your CI pipeline when eval scores regress below acceptable levels.
  </Card>
</CardGrid>

## Quick Example

```typescript
// eval.config.ts
import { defineConfig } from "agent-eval-kit";
import { contains, latency } from "agent-eval-kit/graders";

export default defineConfig({
  suites: [
    {
      name: "smoke",
      target: async (input) => {
        const response = await myAgent(input.prompt);
        return { text: response.text, latencyMs: response.latencyMs };
      },
      cases: "cases/smoke.jsonl",
      defaultGraders: [
        { grader: contains("expected output") },
        { grader: latency(5000) },
      ],
      gates: { passRate: 0.9 },
    },
  ],
});
```

```bash
# Record fixtures (calls your agent once)
agent-eval-kit run --mode=live --record --suite=smoke

# Replay from fixtures (instant, $0)
agent-eval-kit run --mode=replay --suite=smoke

# Watch mode for development
agent-eval-kit run --watch --mode=replay --suite=smoke
```
