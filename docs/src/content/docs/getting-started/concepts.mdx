---
title: Core Concepts
description: Understanding the key concepts in agent-evals
---

## Terminology

agent-evals uses precise terminology. Understanding these terms makes the documentation and API clearer.

| Term | Definition |
|------|-----------|
| **Suite** | A named collection of test cases with a target, graders, and gates |
| **Case** | A single test input with expected output |
| **Trial** | One execution of a case (multiple trials enable statistical analysis) |
| **Run** | A complete execution of a suite — all trials, all grades, all metadata |
| **Grader** | A function that scores a target's output against expected values |
| **Gate** | A threshold that determines if a run passes or fails (e.g., 90% pass rate) |
| **Fixture** | A recorded target response for replay without API calls |

## Execution Modes

### Live mode (default)

Calls your target function for every case. Use this for initial recording and when you need fresh responses.

```bash
agent-evals run --mode=live --suite=smoke
```

### Replay mode

Loads pre-recorded fixtures instead of calling the target. Instant, deterministic, zero cost.

```bash
agent-evals run --mode=replay --suite=smoke
```

### Judge-only mode

Re-grades a previous run with updated graders. Useful when tuning LLM judge prompts.

```bash
agent-evals run --mode=judge-only --run-id=<previous-run-id> --suite=smoke
```

## Graders

Graders are functions that score outputs. agent-evals ships 20 built-in graders in three tiers:

### Deterministic graders
Fast, free, reproducible. Pure functions with no I/O. Examples: `contains`, `exactMatch`, `regex`, `jsonSchema`, `toolCalled`, `toolSequence`, `latency`, `cost`, `safetyKeywords`, `noHallucinatedNumbers`.

### LLM graders
Use an LLM to judge quality. Examples: `llmRubric`, `factuality`, `llmClassify`. These require a judge configuration.

### Composition operators
Combine graders with boolean logic: `all` (conjunction), `any` (disjunction), `not` (negation). These do not short-circuit — they collect all results for complete reporting.

Graders can also be composed with weighted scoring: apply multiple graders per case, weight their scores, and set pass thresholds.

## Gates

Gates are quality thresholds that make eval runs actionable:

```typescript
gates: {
  minPassRate: 0.9,       // 90% of cases must pass
  maxFailCount: 2,        // At most 2 failures allowed
  requireCategories: {
    "critical": 1.0,      // Critical cases must all pass
  },
}
```

Gates are checked after all grading completes. A failed gate means the run's `gateResult.pass` is `false` — useful for CI pipelines.

## Record-Replay (Fixtures)

The record-replay engine is the core differentiator:

1. **Record**: Run in live mode with `--record` to capture target responses as fixtures
2. **Replay**: Run in replay mode to grade from fixtures — no API calls, instant results
3. **Invalidation**: Fixtures are keyed by a config hash. When your suite name or target version changes, fixtures are automatically invalidated
4. **Staleness**: Fixtures older than the configured TTL generate warnings (or errors with `--strict-fixtures`)

This enables **$0 pre-push evals** — record once, replay thousands of times during development.
