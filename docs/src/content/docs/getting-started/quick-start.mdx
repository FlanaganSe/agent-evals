---
title: Quick Start
description: Run your first eval in under 5 minutes
---

## 1. Initialize your project

After [installation](/agent-eval-kit/getting-started/installation/), use the init wizard to scaffold everything:

```bash
npx agent-eval-kit init
```

This creates `eval.config.ts`, starter cases in `cases/smoke.jsonl`, and optionally a GitHub Actions workflow and `AGENTS.md`.

Or create the files manually:

## 2. Create your config

Create `eval.config.ts` in your project root:

```typescript
import { defineConfig } from "agent-eval-kit";
import { contains, latency } from "agent-eval-kit/graders";

export default defineConfig({
  suites: [
    {
      name: "smoke",
      target: async (input) => {
        // Replace with your actual agent/API call
        const response = await fetch("https://api.example.com/chat", {
          method: "POST",
          body: JSON.stringify({ prompt: input.prompt }),
        });
        const data = await response.json();
        return {
          text: data.message,
          latencyMs: data.latencyMs ?? 0,
        };
      },
      cases: "cases/smoke.jsonl",
      defaultGraders: [
        { grader: contains("hello") },
        { grader: latency(5000) },
      ],
      gates: { passRate: 0.8 },
    },
  ],
});
```

## 3. Define test cases

Create `cases/smoke.jsonl`:

```jsonl
{"id": "greeting", "input": {"prompt": "Say hello to the user"}, "expected": {"text": "hello"}}
{"id": "farewell", "input": {"prompt": "Say goodbye to the user"}, "expected": {"text": "goodbye"}}
```

Or use YAML (`cases/smoke.yaml`):

```yaml
- id: greeting
  input:
    prompt: "Say hello to the user"
  expected:
    text: "hello"

- id: farewell
  input:
    prompt: "Say goodbye to the user"
  expected:
    text: "goodbye"
```

## 4. Run in live mode

```bash
# Run against your real API
agent-eval-kit run --suite=smoke
```

This calls your target for each case, grades the output, and prints a summary.

## 5. Record fixtures

```bash
# Record API responses for instant replay
agent-eval-kit run --mode=live --record --suite=smoke
```

Fixtures are saved to `.eval-fixtures/`.

## 6. Replay from fixtures

```bash
# Instant, zero-cost replay
agent-eval-kit run --mode=replay --suite=smoke
```

No API calls are made â€” the recorded responses are replayed and graded.

## 7. Watch mode

```bash
# Re-run on file changes
agent-eval-kit run --watch --mode=replay --suite=smoke
```

## 8. Compare runs

```bash
# List recent runs
agent-eval-kit list

# Compare two runs
agent-eval-kit compare --base=<run-id-1> --compare=<run-id-2>
```

## Next steps

- Learn about [Concepts](/agent-eval-kit/getting-started/concepts/) (modes, graders, fixtures, gates)
- Explore the [Graders Guide](/agent-eval-kit/guides/graders/) for scoring strategies
- Set up [CI Integration](/agent-eval-kit/guides/ci-integration/) for automated quality gates
