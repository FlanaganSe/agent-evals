---
title: Installation
description: Install agent-eval-kit and set up your first eval project
---

## Prerequisites

- **Node.js** 20+ (22 LTS recommended for native TypeScript execution)
- **pnpm**, **npm**, **yarn**, or **bun** package manager

## Install

```bash
# pnpm (recommended)
pnpm add -D agent-eval-kit

# npm
npm install --save-dev agent-eval-kit

# yarn
yarn add --dev agent-eval-kit

# bun
bun add -D agent-eval-kit
```

## Optional dependencies

### MCP Server (AI assistant integration)

If you want to use the MCP server for Claude Code, Cursor, or other AI assistants:

```bash
pnpm add @modelcontextprotocol/sdk
```

## Verify installation

```bash
npx agent-eval-kit --help
```

You should see the available commands listed.

## Initialize a project

The `init` wizard creates a starter config, case files, and optionally a GitHub Actions workflow:

```bash
npx agent-eval-kit init
```

This creates:
- `eval.config.ts` — main configuration file with a framework-specific target stub
- `cases/smoke.jsonl` — 3 starter test cases
- `.eval-fixtures/.gitkeep` — fixture directory
- `.github/workflows/evals.yml` — CI workflow (optional)
- `AGENTS.md` — AI agent boundaries file (optional)

The wizard auto-detects your framework (Vercel AI SDK, LangChain, Mastra, or custom) and package manager.

For non-interactive setup: `npx agent-eval-kit init --yes`

## Package exports

agent-eval-kit provides several subpath exports for targeted imports:

| Import | Description |
|--------|-------------|
| `agent-eval-kit` | Main entry — `defineConfig`, runner, storage, comparison, caching utilities |
| `agent-eval-kit/graders` | All 20 graders, composition operators, scoring, grader types |
| `agent-eval-kit/plugin` | Plugin interface types (`EvalPlugin`, `PluginHooks`) |
| `agent-eval-kit/reporters` | Reporter formatting functions |
| `agent-eval-kit/comparison` | `compareRuns`, `formatComparisonReport` |
| `agent-eval-kit/fixtures` | Fixture loading and management |
| `agent-eval-kit/watcher` | File watcher for watch mode |

## Next steps

Continue to [Quick Start](/agent-eval-kit/getting-started/quick-start/) to run your first eval.
